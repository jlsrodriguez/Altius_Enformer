# -*- coding: utf-8 -*-
"""
================================================================================================================================================================================================================================
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/jaroddy/Altius_Enformer/blob/main/enformer/enformer-usage.ipynb

Copyright 2021 DeepMind Technologies Limited

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

This colab showcases the usage of the Enformer model published in

**"Effective gene expression prediction from sequence by integrating long-range interactions"**

Å½iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley

###This is a modified version of Google's Enformer model for predicting gene expression from a supplied DNA sequence, or from the latest reference genome. It produces pooled(128 bases per value) and unpooled contribution scores for a region of one-hot encoded sequence 114688 bases in length, after being provided a region of 393216 bases in length. The modified program provides a list of transcription start sites that are within 50 kb of the interval. Please see the readme for more details on use.
================================================================================================================================================================================================================================

Modified for use by Altius Institute for Biomedical Science, by Jacob Rodriguez, M.S. see the readme.txt for use information.
Last modified: 7/30/2022


"""### Imports"""

import os
import sys
#Mutes background dialogue concerning the lack of a GPU
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'
import tensorflow as tf
import tensorflow_hub as hub
import gzip
import kipoiseq
from kipoiseq import Interval
import pyfaidx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
#import seaborn as sns
from Bio import SeqIO
#Imports the custom one-hot encoder and fasta_extractor
import support_func as sf

#Paths
#Location of the downloaded model, genome
model_path = '/home/jrodriguez/altius_enformer/tf_models/c444fdff3e183daf686869692c26e00391f6773c'
#Genome Fasta
fasta_file = 'genome.fa'

# Download targets from Basenji2 dataset
# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).
#Loading targets_txt
targets_txt = 'targets_human.txt'
df_targets = pd.read_csv(targets_txt, sep='\t')
df_targets.head(3)

"""### Download files

Download and index the reference genome fasta file

Credit to Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc

Schneider et al 2017 http://dx.doi.org/10.1101/gr.213611.116: Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly
"""

#Location of the reference genome: http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz

#Length of the interval
SEQUENCE_LENGTH = 393216

#Creating Enformer class including its functions

class Enformer:

  def __init__(self, tfhub_url):
    self._model = hub.load(tfhub_url).model
  #Actual predictions
  def predict_on_batch(self, inputs):
    predictions = self._model.predict_on_batch(inputs)
    return {k: v.numpy() for k, v in predictions.items()}
  
  
  @tf.function
  #Contribution Score calculation
  #Modified orignal calculation to do the prediction step in the function below, the previous version predicted the
  #same data twice

  def contribution_input_grad(self, input_sequence,
                              target_mask, output_head='human'):
    

    target_mask_mass = tf.reduce_sum(target_mask)
    with tf.GradientTape() as tape:
      tape.watch(input_sequence)
      prediction_pass = self._model.predict_on_batch(input_sequence)[output_head]
      prediction_pass = tf.cast(prediction_pass, dtype=tf.float64)
      prediction = tf.reduce_sum(
          target_mask[tf.newaxis] *
          prediction_pass) / target_mask_mass
      
    input_grad = tape.gradient(prediction, input_sequence) * input_sequence
    input_grad = tf.squeeze(input_grad, axis=0)
    return tf.reduce_sum(input_grad, axis=-1), prediction_pass

#Vestigial Functions, may be integrated later   
class EnformerScoreVariantsRaw:

  def __init__(self, tfhub_url, organism='human'):
    self._model = Enformer(tfhub_url)
    self._organism = organism
  
  def predict_on_batch(self, inputs):
    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]
    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]

    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)

#Needs to be integrated
class EnformerScoreVariantsNormalized:

  def __init__(self, tfhub_url, transform_pkl_path,
               organism='human'):
    assert organism == 'human', 'Transforms only compatible with organism=human'
    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)
    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:
      transform_pipeline = joblib.load(f)
    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.
    
  def predict_on_batch(self, inputs):
    scores = self._model.predict_on_batch(inputs)
    return self._transform.transform(scores)

#Function below needs to be integrated
class EnformerScoreVariantsPCANormalized:

  def __init__(self, tfhub_url, transform_pkl_path,
               organism='human', num_top_features=500):
    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)
    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:
      self._transform = joblib.load(f)
    self._num_top_features = num_top_features
    
  def predict_on_batch(self, inputs):
    scores = self._model.predict_on_batch(inputs)
    return self._transform.transform(scores)[:, :self._num_top_features]

#Original Fasta Extractor using pyfaidx
class FastaStringExtractor:
    
    def __init__(self, fasta_file):
        self.fasta = pyfaidx.Fasta(fasta_file)
        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}

    def extract(self, interval: Interval, **kwargs) -> str:
        # Truncate interval if it extends beyond the chromosome lengths.
        chromosome_length = self._chromosome_sizes[interval.chrom]
        trimmed_interval = Interval(interval.chrom,
                                    max(interval.start, 0),
                                    min(interval.end, chromosome_length),
                                    )
        # pyfaidx wants a 1-based interval
        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,
                                          trimmed_interval.start + 1,
                                          trimmed_interval.stop).seq).upper()
        # Fill truncated values with N's.
        pad_upstream = 'N' * max(-interval.start, 0)
        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)
        return pad_upstream + sequence + pad_downstream

    def close(self):
        return self.fasta.close()

#Variant generator from the paper *Needs to be integrated*
def variant_generator(vcf_file, gzipped=False):
  """Yields a kipoiseq.dataclasses.Variant for each row in VCF file."""
  def _open(file):
    return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)
    
  with _open(vcf_file) as f:
    for line in f:
      if line.startswith('#'):
        continue
      chrom, pos, id, ref, alt_list = line.split('\t')[:5]
      # Split ALT alleles and return individual variants as output.
      for alt in alt_list.split(','):
        yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,
                                           ref=ref, alt=alt, id=id)

#Kipoiseq one-hot encoder
def one_hot_encode(sequence):
  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)


def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,
                               chr_prefix=''):
  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(
    reference_sequence=FastaStringExtractor(fasta_file))

  for variant in variant_generator(vcf_file, gzipped=gzipped):
    interval = Interval(chr_prefix + variant.chrom,
                        variant.pos, variant.pos)
    interval = interval.resize(sequence_length)
    center = interval.center() - interval.start

    reference = seq_extractor.extract(interval, [], anchor=center)
    alternate = seq_extractor.extract(interval, [variant], anchor=center)

    yield {'inputs': {'ref': one_hot_encode(reference),
                      'alt': one_hot_encode(alternate)},
           'metadata': {'chrom': chr_prefix + variant.chrom,
                        'pos': variant.pos,
                        'id': variant.id,
                        'ref': variant.ref,
                        'alt': variant.alt}}

#Function that finds local transcripts to a particular interval defined by chroms, bounds_lower, bounds_higher
def local_transcripts(transcripts, chroms, bounds_lower, bounds_higher, strand):
   full_pos = pd.DataFrame()
   transcripts['Start'] = pd.to_numeric(transcripts['Start'])
   transcripts['End'] = pd.to_numeric(transcripts['End'])

   #Filtering results by those transcript start sites 50kb away
 
   pos_temp = transcripts[transcripts['Seqid'] == chroms]
   pos_temp_plus = pos_temp[pos_temp['Strand'] == '+']
   pos_temp_negative = pos_temp[pos_temp['Strand'] == '-']
   pos_temp_plus = pos_temp_plus[pos_temp_plus['Start'] <= bounds_lower + 50000 +
                                                  (bounds_higher - bounds_lower)]
   pos_temp_plus = pos_temp_plus[pos_temp_plus['Start'] >= bounds_lower - 50000]
   pos_temp_negative = pos_temp_negative[pos_temp_negative['End'] <= bounds_higher + 50000]
   pos_temp_negative = pos_temp_negative[pos_temp_negative['End'] >= bounds_lower - 50000 - 
                                                  (bounds_higher-bounds_lower)]
   full_pos = full_pos.append(pos_temp_plus)
   full_pos = full_pos.append(pos_temp_negative)
   full_pos = full_pos.reset_index(drop=True)
   
   return full_pos
   
#Function for plotting tracks shown in the paper *Needs to be integrated*
def plot_tracks(tracks, interval, height=1.5):
  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)
  for ax, (title, y) in zip(axes, tracks.items()):
    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)
    ax.set_title(title)
    sns.despine(top=True, right=True, bottom=True)
  ax.set_xlabel(str(interval))
  plt.tight_layout()

"""## Make predictions for a genetic sequence"""
#Load transcripts
transcripts = pd.read_csv('pos_report.csv')
#Load Regions of Interest
ROIs = pd.read_csv('ROIs.csv')
#Initialize the answer variable defining the type of analysis performed
question_0 = 'X'
target = 'X'
#Load the model
model = Enformer(model_path)
#Initialize output for finding ROI or TSS location 
orig_ROI = pd.DataFrame(index=range(1), columns = ['Start', 'End'])

TSS_loc = [0, 0, 0]

TSS_data = pd.DataFrame(index=range(1), columns = ['Start', 'Mid', 'End'])

bed_contr = pd.DataFrame(index=range(896), columns = ['Start', 'End', 'Score'])
#Ask whether user is interested in testing variants (Y) or testing regions of interest from the reference genome (N)
while (question_0 != 'Y') and (question_0 != 'N'):

   msg = "Are you obtaining predictions for variants?"
   question_0 = input("%s (Y/N) " % msg)

   if str(question_0) == 'Y':

      raw_sequences, handles = sf.fasta_extractor(test_fasta)

      one_hot = sf.one_hot_encoder(raw_sequences)

   elif str(question_0) == 'N':

      fasta_extractor = FastaStringExtractor(fasta_file)

   else:
   
      question_0 = input("%s (Y/N) " % msg)

#Asking what line of targets_human.txt user wants to choose; what type of cells are you making predictions for

while (target == 'X'):

   msg = "What line of targets_txt are you interested in obtaining predictions for?"

   target = int(input("%s (####) " % msg))

"""## Contribution scores calculation##"""
#If the user is only interested in the reference genome
if str(question_0) == 'N':
   #For each sample in the ROI.csv
   for x in range(len(ROIs)):

       print(x)

       #Temporary variables

       interval = str(ROIs['fasta_label'][x])

       begin = float(ROIs['bounds_lower'][x])

       end = float(ROIs['bounds_higher'][x])
       
       strand = str(ROIs['strand'][x])

       #Finding the local transcripts
      
       loc_trans = local_transcripts(transcripts, interval, begin, end, strand) 

       #Making the bed file
       
       bed = pd.DataFrame(columns = ["chrom","chromStart","chromEnd","strand"])

       bed["chrom"] = loc_trans["Seqid"]

       bed["chromStart"] = loc_trans["Start"]

       bed["chromEnd"] = loc_trans["End"]

       bed["strand"] = loc_trans["Strand"]

       bed = bed.to_numpy()

       bed[:,1] = bed[:,1].astype(np.int32)
     
       bed[:,2] = bed[:,2].astype(np.int32)

       bed_full = np.array([bed[:,0],bed[:,1],bed[:,2],bed[:,3]])

       bed_full2 = np.transpose(bed_full)

       #Defining the target interval
       target_interval = kipoiseq.Interval(interval, int((begin-57344)), int((begin+57344)))  # @param
       #One-hot encode the relevant interval
       sequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))
       #Make predictions for the target_mask
       #predictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]
       
       target_mask = np.zeros((896,5313))

       #Define the region for which contributions are being made
       for idx in [447, 448, 449]:
           #4760 corresponds to the CAGE: CD8+ Cells track (more can be found in targets_human.txt) 
           target_mask[idx, target] = 1
       
       sequence_one_hot = sequence_one_hot[tf.newaxis]  
       #Calculate Contribution scores
       contribution_scores, predictions_cs = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask)
       contribution_scores = contribution_scores.numpy()

       #Calculate pooled scores
       pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]

       predictions_cs = predictions_cs.numpy()

       for g in range(len(pooled_contribution_scores)):

           bed_contr['Start'][g] = int(begin) - 57344 + (128*(g-1))
           bed_contr['End'][g] = int(begin) - 57344 + (128*g)
           bed_contr['Score'][g] = pooled_contribution_scores[g]

       bed_contr = bed_contr.to_numpy()
       print('step1')
       #Save contribution scores, predictions and local transcripts

       np.savetxt('./predictions/' + "ROI_contribution_scores_" + str(x) + ".bed", bed_contr, fmt='%s', delimiter="\t")

      # np.savetxt('./predictions/' + "predictions" + str(x) + ".csv", predictions_cs, delimiter=",")  TODO: Make this 2-dim

       np.savetxt('./predictions/' + 'ROI_local_transcripts_' + str(x) + '.bed', bed_full2,fmt='%s', delimiter="\t")
      
#Option A: Shift ROI to be centered at the TSS, then find original ROI location via orig_ROI
############################################################################################################################################################
      # for transcript in range(len(loc_trans)):
          
       #   print("TSS " + str(transcript))

        #  orig_ROI['Start'][0] = round((begin-(bed_full[1][transcript]-57344))/128)
#          orig_ROI['End'][0] = round((begin-(bed_full[1][transcript]-57344))/128)


 #         target_interval = kipoiseq.Interval(str(bed_full[0,transcript]), int(bed_full[1, transcript])-57344, int(bed_full[1, transcript])+57344)

  #        sequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))

   #       target_mask = np.zeros((896,5313))

    #      sequence_one_hot = sequence_one_hot[tf.newaxis]

     #     for idx in [447, 448, 449]:

      #        target_mask[idx, target] = 1

       #   contribution_scores, predictions_cs = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask)
        #  contribution_scores = contribution_scores.numpy()

         # predictions_cs = predictions_cs.numpy()

          #pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]

         # np.savetxt('./predictions/' + 'OptA_TSS' + str(transcript) + '.csv', pooled_contribution_scores, delimiter=',')         
# print(pooled_contribution_scores)
###############################################################################################################################################################

#Option B: Identify idx positions corresponding to the TSS, calculate contributions to that region
################################################################################################################################################################
      # for b in range(len(loc_trans)):

       #    pos = round((bed_full[1][b]-(begin-57344))/128)

        #   TSS_loc = [pos-1, pos, pos+1]
          
         #  target_interval = kipoiseq.Interval(interval, int((begin-57344)), int((begin+57344)))

         #  sequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))    
         #  target_mask = np.zeros((896, 5313))
           
         #  sequence_one_hot = sequence_one_hot[tf.newaxis]
 
          # for idx in TSS_loc:

           #    target_mask[idx, target] = 1
 
          # contribution_scores, predictions_cs = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask)
          # contribution_scores = contribution_scores.numpy()
          # predictions_cs = predictions_cs.numpy()
           
          # pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]
  
          # np.savetxt('./predictions/' + 'OptB_TSS' + str(b) + '.csv', pooled_contribution_scores, delimiter=',')
         #  print(pooled_contribution_scores)
#####################################################################################################################################################################

#Option C:Report all data in option B in one matrix where a single column is one TSS
#####################################################################################################################################################################
      
       #Initialize dataframe holding the idx positions
       TSS_data = pd.DataFrame(index = range(len(loc_trans)), columns = ['Start', 'Mid', 'End'])

       #Create dataframe for contribution scores
       TSS_contr_scores = pd.DataFrame(index = np.arange(896), columns = np.arange(len(loc_trans))) 
      
       #Interval centered around the ROI
       target_interval = kipoiseq.Interval(interval, int((begin-57344)), int((begin+57344))) 

       #One-hot area around the interval
       sequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))

       #Create extra axis for the prediction step
       sequence_one_hot = sequence_one_hot[tf.newaxis]
 
       #Fill out the TSS_data object with positions starting at the beginning of the ROI, with the TSS starting position
       #converted to positons from 0-895, where 0 is the beginning of the ROI and the Mid position be the beginning of the TSS
       for c in range(len(loc_trans)):
  
           pos = round((bed_full[1][c]-(begin-57344))/128)
          
           TSS_data['Start'][c] = pos-1
           TSS_data['Mid'][c] = pos
           TSS_data['End'][c] = pos+1
       

       #Creating contribution score containing bed file for each TSS and then filling it out
       for d in range(len(TSS_data)):
           
           bed_contr = pd.DataFrame(index=range(896), columns=['Start', 'End', 'Score'])
           target_mask = np.zeros((896, 5313))
           
           for idx in TSS_data.iloc[d, :]:
               
               target_mask[idx, target] = 1

           contribution_scores, predictions_cs = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask)
           contribution_scores = contribution_scores.numpy()
           predictions_cs = predictions_cs.numpy()

           pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]

           TSS_contr_scores.iloc[:,d] = pooled_contribution_scores
           
           for g in range(len(pooled_contribution_scores)):
               
               bed_contr['Start'][g] = int(begin)-57344+(128*(g-1))
               bed_contr['End'][g] = int(begin)-57344+(128*g)
               bed_contr['Score'][g] = pooled_contribution_scores[int(g)] 
      
           bed_contr = bed_contr.to_numpy()
           
           #Save scored bed file for each TSS
           np.savetxt('./predictions/' + 'scored_positions_' + str(d) + '.bed', bed_contr, fmt='%s', delimiter="\t")
       
      
#Similar functions to above but with variants *Needs to be integrated and completed*       
elif str(question_0) == 'Y':

#one-hot encoded fasta files in the one_hot dictionary
     #Look at kipoiseq's variant creator
     #Find differences between variant predictions and wt-predictions
     #Locate local transcripts, center interval on each TSS
     #Provide list of positions, get back (for each transcript within 50kb) contribution score of region of interest to that tran     script
     #Generate predictions (once for 100kb region), contribution
     for y in range(len(one_hot)):

         predictions = model.predict_on_batch(one_hot[y])['human'][0]

         target_mask = np.zeros_like(predictions)

         print(predictions.shape)

         for idx in [447, 448, 449]:

             target_mask[idx, 4760] = 1
          
         contribution_scores = model.contribution_input_grad(one_hot[y].astype(np.float32), target_mask).numpy()
         pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]
         
         scores = pooled_contribution_scores
         #Save score files with fasta handle as the name
       #  np.savetxt('./predictions/' + str(handles[y]) + "_cont_scores.csv", scores, delimiter=",")





